<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="width=device-width, initial-scale=1">
    <meta name="description" content="Inverting a Pendulum with Q-Learning"/>
    <title>Inverting a Pendulum with Q-Learning</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
  MathJax = {tex: {
      tags: 'ams'  // should be 'ams', 'none', or 'all'
    }
  };
  </script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2>Inverting a Pendulum with model-free reinforcement learning (Q-Learning)</h2>
            <h4 style="color:#6e6e6e;"> Reinforcement Learning and Optimal Control for Robotics (ROB-GY 6323) Fall 2022 </h4>
            <hr>
            <h6> <a href="https://github.com/RohanG9929" target="_blank">Rohan Gangakhedkar</a><sup>1</sup>, 
                 
            <p> <sup>1</sup>New York Univeristy &nbsp;&nbsp; 
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/pdf/2104.00680.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div> -->
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/RohanG9929/Inverting-a-Pendulum-with-Q-Learning" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/LoFTR-suppmat.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/LoFTR_IMC21.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> IMC-Report </a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <h6 style="color:#8899a5" class="text-center"> 
              TL;DR: The system was able to learn a policy for an inverted pendulum model to make it do a swing-up motion. 
            </h6>
              <br>
              
              <video id="demo" width="50%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/Control1.mp4" type="video/mp4">
              </video>
              <div><b style="color:#fd5638; font-size:large" id="demo-warning"></b>
              <br>
          <p class="text-justify">
            The Q-learning algorithm took 6000 episodes to learn how to invert the pendulum.
            As can be seen in the video above, there is one back and forth swing before the pendulum can lift up and remain inverted.</p>
        </div>
      </div>
    </div>
  </section>
  <br>


 <!-- pipeline -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Approach</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" width="15% "src="images/pendulum.png" alt="Pendulum">
                <hr>
                <p class="text-justify">
                  The Pendulum is depicted in the image above.
                  <br><br>

                  In this scenario the inverted pendulum has a limit on the maximum torque it can apply, 
                  therefore it is necessary for the pendulum to do a few "back and forth" motions to be able to 
                  reach the inverted position (\(\theta=\pi\)) from the standing still non-inverted position (\(\theta=0\)).
                  <br>
                  The state will be written as \(x = \begin{pmatrix} \theta \\ \omega \end{pmatrix}\) as the vector of states of the system. 
                  And the dynamics will be time-discretized , and refer to \(x_n\) as the state at time \(t = n \Delta t\) (assuming discretization time \(\Delta t\))
                  <br>
                  The following discounted cost function needs to be minimized.
                  \(\sum_{i=0}^{\infty} \alpha^i g(x_i, u_i)\) where 
                  \begin{equation}
                  g(x_i, u_i) = (\theta-\pi)^2 + 0.01 \cdot \dot{\theta}_i^2 + 0.0001 \cdot u_i^2 \qquad \textrm{and} \qquad\alpha=0.99
                  \end{equation}

                  This cost mostly penalizes deviations from the inverted position but also encourages small velocities and control.
                  
                  <br><br>
                </p>

                <h4>Using a control of \(u \in \{-4,0,4\}\)</h4>
                <p class="text-justify">
                  Below shows the outcome of the Q-Learning process when the control is \(u \in \{-4,0,4\}\).
                  <br>
                  <center>
                    <video id="demo" width="50%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                      <source src="images/Control1.mp4" type="video/mp4">
                    </video>
                  </center>
                  As can be seen in the video above, there is one back and forth swing before the pendulum can lift up and remain inverted.
                  <br>
                  <img class="img-fluid" width="50% "src="images/LearningProgress_1.jpg" alt="Learning">
                  <br>
                  Shown in the graph is the cost per episode (shown in blue), and the average cost (shown in orange). The graph
                  shows the decrease in cost as the Q-learning is taking place. The Q-learning algorithm for this control, took 6000 episodes to 
                  learn how to invert the pendulum.
                  <br><br>
                  Below are the value functions and policy for this control scheme.
                  <br><br>
                  <img class="img-fluid" width="30% "src="images/Value_1.jpg" alt="Value Function">
                  <img class="img-fluid" width="30% "src="images/Policy_2.jpg" alt="Policy">
                  <br>
                  The policy is the optimal action to take at each state. The value function is the associated value at each state when performing the optimal action. 
           
                </p>



                <h4>Using a control of \(u \in \{-5,0,5\}\)</h4>
                <p class="text-justify">
                  Controller that ensures the robot reachs a vertical orientation, \(\theta = \frac{\pi}{2} \) at the location \(x = 3 , y = 3\) at time \(t = 5\) starting from the origin. 
                  During the remainder of the motion, the robot trys to stay close to the origin. 
                  <br>
                  <center>
                    <video id="demo" width="50%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                      <source src="images/Control2.mp4" type="video/mp4">
                    </video>
                  </center>
                  The extra effort that the controller can apply allows for a quicker learning in the Q-table. 
                  This can be seen with the lower final cost (around ~ 200) in the learning progress.
                  And because of this the policy is able to invert the pendulum quicker than the \(u \in \{-5,0,5\}\) case. 
                  With no back and forth necessary for the pendulum, only one swing before inverting.

                  <br>
                  <img class="img-fluid" width="50% "src="images/LearningProgress_2.jpg" alt="Learning">
                  <br>
                  The Q-learning algorithm for this control, again took 6000 episodes to 
                  learn how to invert the pendulum. This time learning slightly more.
                  <br><br>
                  Below are the value functions and policy for this control scheme.
                  <br><br>
                  <img class="img-fluid" width="30% "src="images/Value_2.jpg" alt="Value Function">
                  <img class="img-fluid" width="30% "src="images/Policy_2.jpg" alt="Policy">
                  <br>
           
                </p>
              
                <br><br>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- ack -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            I would like to specially thank the Professor Ludovic Righetti for allowing me to showcase this project on my website.
          </p>
          <hr>
      </div>
    </div>
  </div>

  <!-- rec -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Recommendations to other works by me</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            Welcome to checkout my  (<a href="http://zju3dv.github.io/neuralrecon/">NeuralRecon</a>) and human reconstruction (<a href="http://zju3dv.github.io/neuralbody">NeuralBody</a> and <a href="http://zju3dv.github.io/Mirrored-Human">Mirrored-Human</a>) in CVPR 2021.
          </p>
      </div>
    </div>
  </div> -->

  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  
</body>
</html>